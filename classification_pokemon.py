# -*- coding: utf-8 -*-
"""Classification_Pokemon_BARBOSA_LASGLEIZES_RIBAS_THIRE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rN0liEsBJysYJz2cIQ_49b0VYBTu1SHZ

# CNN simple

# Classification images de base
"""

from google.colab import drive
import pandas as pd
import os
from PIL import Image
import cv2
import shutil
import random

drive.mount('/content/gdrive')

# Chemin vers le dossier contenant les classes
folders = "/content/gdrive/MyDrive/SegmentationDePokemons"

# Dossiers pour les ensembles d'entraînement et de test
train_folder = "/content/gdrive/MyDrive/SegmentationDePokemons/train"
test_folder = "/content/gdrive/MyDrive/SegmentationDePokemons/test"

# Créer les dossiers s'ils n'existent pas déjà
os.makedirs(train_folder, exist_ok=True)
os.makedirs(test_folder, exist_ok=True)

pokemon_list = ['Pikachu', 'Charizard', 'Bulbasaur', 'Alakazam', 'Blastoise', 'Eevee', 'Gyarados', 'Jigglypuff', 'Lapras', 'Mew']
# Parcourir les 10 classes
for i, class_folder in enumerate(os.listdir(folders)):
    if class_folder in pokemon_list:
        class_path = os.path.join(folders, class_folder)

        # Vérifier si c'est un dossier
        if os.path.isdir(class_path):

            # Créer des sous-dossiers pour chaque classe dans les dossiers d'entraînement et de test
            train_class_folder = os.path.join(train_folder, class_folder)
            test_class_folder = os.path.join(test_folder, class_folder)

            os.makedirs(train_class_folder, exist_ok=True)
            os.makedirs(test_class_folder, exist_ok=True)

            # Liste des fichiers dans le dossier
            files = os.listdir(class_path)

            # Trier les fichiers par ordre alphabétique
            files.sort()

            # Diviser les fichiers en ensembles d'entraînement et de test (80% train, 20% test)
            random.shuffle(files)
            split_index = int(0.8 * len(files))
            train_files = files[:split_index]
            test_files = files[split_index:]

            # Copier les fichiers dans les dossiers appropriés
            for file in train_files:
                src_path = os.path.join(class_path, file)
                new_file_name = f"{class_folder}-{file}"
                dest_path = os.path.join(train_class_folder, new_file_name)
                shutil.copy(src_path, dest_path)

            for file in test_files:
                src_path = os.path.join(class_path, file)
                new_file_name = f"{class_folder}-{file}"
                dest_path = os.path.join(test_class_folder, new_file_name)
                shutil.copy(src_path, dest_path)

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

from tensorflow.keras.preprocessing.image import ImageDataGenerator
import keras

# Définir la nouvelle taille des images en entrée
img_size = (200, 200, 3)

model = Sequential([
    Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=img_size),
    Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=2),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2), strides=2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(10, activation='softmax')
])

adam = keras.optimizers.Adam(lr=0.001)

# Compiler le modèle
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Afficher un résumé du modèle
model.summary()

# Créer des générateurs d'images pour l'entraînement
train_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
    '/content/gdrive/MyDrive/SegmentationDePokemons/train',
    target_size=(200,200),
    batch_size=32,
    class_mode='categorical'
)

# Entraîner le modèle
model.fit(train_generator, epochs=3)

model.save("/content/gdrive/MyDrive/SegmentationDePokemons/classification_images_classiques.h5")

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, confusion_matrix

# Définir la taille des images en entrée
img_size = (200, 200, 3)

# Créer des générateurs d'images pour l'ensemble de test
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    '/content/gdrive/MyDrive/SegmentationDePokemons/test',
    target_size=(200, 200),
    batch_size=32,
    class_mode='categorical',
    shuffle=False  # Ne pas mélanger les données pour évaluer correctement les métriques
)

# Évaluer le modèle sur l'ensemble de test
evaluation = model.evaluate(test_generator)

# Afficher les métriques d'évaluation
print("Loss:", evaluation[0])
print("Accuracy:", evaluation[1])

# Prédictions sur l'ensemble de test
predictions = model.predict(test_generator)

# Convertir les prédictions en classes
predicted_classes = [round(pred[0]) for pred in predictions]

# Obtenir les vraies classes
true_classes = test_generator.classes

# Afficher la matrice de confusion
conf_matrix = confusion_matrix(true_classes, predicted_classes)
print("Matrice de confusion:")
print(conf_matrix)

# Afficher le rapport de classification
class_labels = list(test_generator.class_indices.keys())
class_report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print("Rapport de classification:")
print(class_report)

"""# Classification images segmentées par U-Net global"""

def train_test(folders):
  # Dossiers pour les ensembles d'entraînement et de test
  train_folder = folders+"/train"
  test_folder = folders+"/test"

  # Créer les dossiers s'ils n'existent pas déjà
  os.makedirs(train_folder, exist_ok=True)
  os.makedirs(test_folder, exist_ok=True)

  # Parcourir les 10 classes
  for class_folder in os.listdir(folders):
        class_path = os.path.join(folders, class_folder)

        # Vérifier si c'est un dossier de classe
        if os.path.isdir(class_path) and class_folder!=class_folder.lower():

            # Créer des sous-dossiers pour chaque classe dans les dossiers d'entraînement et de test
            train_class_folder = os.path.join(train_folder, class_folder)
            test_class_folder = os.path.join(test_folder, class_folder)

            os.makedirs(train_class_folder, exist_ok=True)
            os.makedirs(test_class_folder, exist_ok=True)

            # Liste des fichiers dans le dossier
            files = os.listdir(class_path)

            # Trier les fichiers par ordre alphabétique
            files.sort()

            # Diviser les fichiers en ensembles d'entraînement et de test (80% train, 20% test)
            random.shuffle(files)
            split_index = int(0.8 * len(files))
            train_files = files[:split_index]
            test_files = files[split_index:]

            # Copier les fichiers dans les dossiers appropriés
            for file in train_files:
                src_path = os.path.join(class_path, file)
                new_file_name = f"{class_folder}-{file}"
                dest_path = os.path.join(train_class_folder, new_file_name)
                shutil.copy(src_path, dest_path)

            for file in test_files:
                src_path = os.path.join(class_path, file)
                new_file_name = f"{class_folder}-{file}"
                dest_path = os.path.join(test_class_folder, new_file_name)
                shutil.copy(src_path, dest_path)

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

from tensorflow.keras.preprocessing.image import ImageDataGenerator
import keras

# Définir la nouvelle taille des images en entrée
img_size = (200, 200, 3)

model = Sequential([
    Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=img_size),
    Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=2),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2), strides=2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(10, activation='softmax')
])

adam = keras.optimizers.Adam(lr=0.001)

# Compiler le modèle
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Afficher un résumé du modèle
model.summary()

segmented_folder="/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations"
train_test(segmented_folder)

# Créer des générateurs d'images pour l'entraînement
train_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
    '/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations/train',
    target_size=(200,200),
    batch_size=32,
    class_mode='categorical'
)

# Entraîner le modèle
model.fit(train_generator, epochs=6)

model.save("/content/gdrive/MyDrive/SegmentationDePokemons/classification_segmentees_unet_global.h5")

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, confusion_matrix

# Définir la taille des images en entrée
img_size = (200, 200, 3)

# Créer des générateurs d'images pour l'ensemble de test
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    '/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations/test',
    target_size=(200, 200),
    batch_size=32,
    class_mode='categorical',
    shuffle=False  # Ne pas mélanger les données pour évaluer correctement les métriques
)

# Évaluer le modèle sur l'ensemble de test
evaluation = model.evaluate(test_generator)

# Afficher les métriques d'évaluation
print("Loss:", evaluation[0])
print("Accuracy:", evaluation[1])

# Prédictions sur l'ensemble de test
predictions = model.predict(test_generator)

# Convertir les prédictions en classes
predicted_classes = [round(pred[0]) for pred in predictions]

# Obtenir les vraies classes
true_classes = test_generator.classes

# Afficher la matrice de confusion
conf_matrix = confusion_matrix(true_classes, predicted_classes)
print("Matrice de confusion:")
print(conf_matrix)

# Afficher le rapport de classification
class_labels = list(test_generator.class_indices.keys())
class_report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print("Rapport de classification:")
print(class_report)

"""# Classification images segmentées par U-Net entraîné spécialement sur ces dix classes"""

segmented_folder="/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations2"
train_test(segmented_folder)

# Créer des générateurs d'images pour l'entraînement
train_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
    '/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations2/train',
    target_size=(200,200),
    batch_size=32,
    class_mode='categorical'
)

# Entraîner le modèle
model.fit(train_generator, epochs=6)

model.save("/content/gdrive/MyDrive/SegmentationDePokemons/classification_segmentees_unet_specialise.h5")

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, confusion_matrix

# Définir la taille des images en entrée
img_size = (200, 200, 3)

# Créer des générateurs d'images pour l'ensemble de test
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    '/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations2/test',
    target_size=(200, 200),
    batch_size=32,
    class_mode='categorical',
    shuffle=False  # Ne pas mélanger les données pour évaluer correctement les métriques
)

# Évaluer le modèle sur l'ensemble de test
evaluation = model.evaluate(test_generator)

# Afficher les métriques d'évaluation
print("Loss:", evaluation[0])
print("Accuracy:", evaluation[1])

# Prédictions sur l'ensemble de test
predictions = model.predict(test_generator)

# Convertir les prédictions en classes
predicted_classes = [round(pred[0]) for pred in predictions]

# Obtenir les vraies classes
true_classes = test_generator.classes

# Afficher la matrice de confusion
conf_matrix = confusion_matrix(true_classes, predicted_classes)
print("Matrice de confusion:")
print(conf_matrix)

# Afficher le rapport de classification
class_labels = list(test_generator.class_indices.keys())
class_report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print("Rapport de classification:")
print(class_report)

"""# Transfer learning

# Classification images de base
"""

from google.colab import drive
import pandas as pd
import os
from PIL import Image
import cv2
import shutil
import random
import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint

drive.mount('/content/gdrive')

ex_folder = "/content/gdrive/MyDrive/SegmentationDePokemons"
parent_folder = "/content/gdrive/Shareddrives/SegmentationdePokemons/Pokemons"

pokemon_list = ['Pikachu', 'Charizard', 'Bulbasaur', 'Alakazam', 'Blastoise', 'Eevee', 'Gyarados', 'Jigglypuff', 'Lapras', 'Mew']

for class_folder in os.listdir(ex_folder):
    if class_folder in pokemon_list:
        class_path = os.path.join(ex_folder, class_folder)
        os.makedirs(os.path.join(parent_folder, class_folder), exist_ok=True)
        files = os.listdir(class_path)

        for file in files:
            src_path = os.path.join(class_path, file)
            new_file_name = f"{class_folder}-{file}"
            dest_path = os.path.join(parent_folder, class_folder, new_file_name)
            shutil.copy(src_path, dest_path)

size = 200
bs = 32      # Batch size

train_data_gen = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1, zoom_range=0.1, shear_range=0.1, brightness_range=[0.8,1.2], validation_split=0.15, preprocessing_function=preprocess_input)
train_data = train_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='training')

validation_data_gen = ImageDataGenerator(validation_split=0.15, preprocessing_function=preprocess_input)
validation_data = validation_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='validation')

test_data_gen = ImageDataGenerator(validation_split=0.10, preprocessing_function=preprocess_input)
test_data = test_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', subset='validation', shuffle=False)

shape = train_data.image_shape
print(shape)
k = train_data.num_classes
train_samples = train_data.samples
validation_samples = validation_data.samples

input = Input(shape=shape)

basemodel = InceptionV3(include_top=False, weights='imagenet', input_shape=shape, pooling='avg')   # Basemodel est InceptionV3 avec les poids pré-entraînés sur le dataset imagenet
basemodel.trainable = False                                                                        # Bloque les poids de toutes les couches du CNN

x = basemodel(input)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(k, activation='softmax')(x)

model = Model(input,output)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
stop = EarlyStopping(monitor='val_loss', patience=4, mode='min', restore_best_weights=True)
checkpoint = ModelCheckpoint(filepath='{val_loss:.4f}-weights-{epoch:02d}.hdf5', monitor='val_loss', mode='min', save_best_only=True)
model.summary()

ep = 50                      # Nombre epochs
spe = train_samples/bs       # Étapes par epochs
vs = validation_samples/bs   # Étapes de validation

r = model.fit(train_data, validation_data=validation_data, steps_per_epoch=spe, validation_steps=vs, epochs=ep, callbacks=[stop,checkpoint])

model.save("/content/gdrive/Shareddrives/SegmentationdePokemons/classification_model_images.h5")

import matplotlib.pyplot as plt

model.evaluate(validation_data)
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
plt.show()
plt.plot(r.history['accuracy'], label='accuracy')
plt.plot(r.history['val_accuracy'], label='val_accuracy')
plt.legend()
plt.show()

pred = model.predict(test_data).argmax(axis=1)
labels = list(train_data.class_indices.keys())

from sklearn.metrics import classification_report

print(classification_report(test_data.classes, pred))

rand = np.random.randint(low=0, high=test_data.samples, size=5)

for n in rand:
  true_index = test_data.classes[n]
  predicted_index = pred[n]
  img = cv2.imread(test_data.filepaths[n])
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  plt.imshow(img)
  plt.title('True label={}  Predicted label={}'.format(labels[true_index], labels[predicted_index]))
  plt.show()

"""# Classifications avec les images segmentées par U-Net"""

import random
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

def classify(parent_folder,i):
  size = 200
  bs = 32      # Batch size
  if i == 1:
    var = "full"
  elif i == 2:
    var = "10_classes"
  elif i == 3:
    var = "full_cleaned"
  else:
    var = "10_classes_cleaned"

  train_data_gen = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1, zoom_range=0.1, shear_range=0.1, brightness_range=[0.8,1.2], validation_split=0.15, preprocessing_function=preprocess_input)
  train_data = train_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='training')

  validation_data_gen = ImageDataGenerator(validation_split=0.15, preprocessing_function=preprocess_input)
  validation_data = validation_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='validation')

  test_data_gen = ImageDataGenerator(validation_split=0.10, preprocessing_function=preprocess_input)
  test_data = test_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', subset='validation', shuffle=False)

  shape = train_data.image_shape
  print(shape)
  k = train_data.num_classes
  train_samples = train_data.samples
  validation_samples = validation_data.samples

  input = Input(shape=shape)

  basemodel = InceptionV3(include_top=False, weights='imagenet', input_shape=shape, pooling='avg')   # Basemodel est InceptionV3 avec les poids pré-entraînés sur le dataset imagenet
  basemodel.trainable = False                                                                        # Bloque les poids de toutes les couches du CNN

  x = basemodel(input)
  x = Dense(1024, activation='relu')(x)
  x = Dropout(0.2)(x)
  output = Dense(k, activation='softmax')(x)

  model = Model(input,output)
  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
  stop = EarlyStopping(monitor='val_loss', patience=4, mode='min', restore_best_weights=True)
  checkpoint = ModelCheckpoint(filepath='{val_loss:.4f}-weights-{epoch:02d}.hdf5', monitor='val_loss', mode='min', save_best_only=True)
  model.summary()
  ep = 50                      # Nombre epochs
  spe = train_samples/bs       # Étapes par epochs
  vs = validation_samples/bs   # Étapes de validation

  r = model.fit(train_data, validation_data=validation_data, steps_per_epoch=spe, validation_steps=vs, epochs=ep, callbacks=[stop,checkpoint])
  model.save("/content/gdrive/Shareddrives/SegmentationdePokemons/classification_model_unet_"+var+".h5")
  model.evaluate(validation_data)
  plt.plot(r.history['loss'], label='loss')
  plt.plot(r.history['val_loss'], label='val_loss')
  plt.legend()
  plt.show()
  plt.plot(r.history['accuracy'], label='accuracy')
  plt.plot(r.history['val_accuracy'], label='val_accuracy')
  plt.legend()
  plt.show()
  pred = model.predict(test_data).argmax(axis=1)
  labels = list(train_data.class_indices.keys())
  print(classification_report(test_data.classes, pred))
  rand = np.random.randint(low=0, high=test_data.samples, size=5)

  for n in rand:
    true_index = test_data.classes[n]
    predicted_index = pred[n]
    img = cv2.imread(test_data.filepaths[n])
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.imshow(img)
    plt.title('True label={}  Predicted label={}'.format(labels[true_index], labels[predicted_index]))
    plt.show()

parent_folder = "/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations"
classify(parent_folder,1)

classify("/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations2",2)

"""# Après nettoyage des données"""

classify("/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations_clean",3)

classify("/content/gdrive/Shareddrives/SegmentationdePokemons/Segmentations2_clean",4)